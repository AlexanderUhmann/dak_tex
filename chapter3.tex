\chapter{Тестирование алгоритмов машинного и глубокого обучения на данных по кредитному скорингу}

В предыдущей главе было выяснено, что алгоритмы градиентного бустинга и нейронных сетей не имеют эффектов переобучения и недообучения. В случае градиентного бустинга используются гиперпараметры по умолчанию, результаты метрик которых не ниже метрик других алгоритмов. Это означает, что не требуется поиск гиперпараметров для данного алгоритма и можно остановиться на гиперпараметрах по умолчанию. Идентичная ситуация возникает у нейронной сети с отличием в том, что у нее метрики точности немного хуже, чем у алгоритма градиентного бустинга. Поэтому в этой главе будут подобраны гиперпараметры для алгоритмов деревьев решений и случайного леса, поскольку они обладают эффектом переобучения. Основным инструментом для поиска оптимальных гиперпараметров используются инструменты $GridSearchCV$\footnote{\cite{GridSearchCV} (дата обращения: 25.11.2025).} и $RandomizedSearchCV$ \footnote{\cite{RandomizedSearchCV} (дата обращения: 25.11.2025).}.
\footnote{\cite{Markov2022} (дата обращения: 25.11.2025).}
 Также в конце главы будут проанализированы прибыль и убыток банка на основании предсказаний четырех рассмотренных алгоритмов.

\section{Подбор гиперпараметров для алгоритмов машинного обучения при оценке возврата кредита}

Гиперпараметры -- это такие параметры модели, которые задаются до начала работы модели. Правильно подобранные параметры позволяют улучшить качество предсказаний моделей. Каждому алгоритму МО присуще собственные наборы гиперпараметров, которые будут рассматриваться ниже. Существуют два метода подбора гиперпараметров:

\begin{enumerate}
	\item $GridSearchCV$ -- метод подбора оптимальных гиперпараметров для модели с помощью перебора всех возможных комбинаций из заданного набора, что является ресурсозатратным способом.
	\item $RandomizedSearchCV$ -- метод, позволяющий выбирать количество случайных комбинаций из заданного набора гиперпараметров. Является наиболее эффективным методом по времени при работе с большими данными.
\end{enumerate}

\subsection{Деревья решений}

Для алгоритма классификации решающих деревьев ($Decision\ Tree\ Classifier$) основными гиперпараметрами являются:

\begin{enumerate}
	\item $criterion$ -- критерий качества разбиения, определяющая показатель, по которому алгоритм лучше классифицирует клиентов. \begin{enumerate}
	\item [a)] $gini$ -- индекс Джини\footnote{Джини К. (1884-1965 гг.) -- итальянский статистик, экономист, социолог и демограф.}, измеряющий неоднородность двух классов (0 и 1) внутри узла. Если индекс равен 0, значит в узле содержатся объекты одного класса. Чем индекс ближе к 0, тем лучше алгоритм определяет класс клиента. Индекс Джини в алгоритме $Decision\ Tree$ используется по умолчанию, поскольку быстрее вычисляется;
	\item [b)] $entropy$ -- энтропия Шеннона\footnote{Шеннон К.Э. (1916-2001 гг.) -- американский инженер, криптоаналитик и математик, заложил основы теории информации.}, измеряющий степень смешанности классов. Высокая энтропия свидетельствует о высокой неоднородности классов, чем ниже энтропия, тем узел более однородный. Алгоритм выбирает разбиения с наиболее низкой энтропией, т.е. содержатся большинство объектов одного класса;
	\item [ с)] $log\_loss$ -- логарифмическая функция потерь. В данном случае алгоритм не просто выбирает класс, а оценивает их вероятности. За каждый неверный прогноз вводит штраф.
	\end{enumerate}
\end{enumerate}

\subsection{Случайный лес}
\subsection{Градиентный бустинг}
\subsection{Нейронная сеть}
\section{Ограничения и возможности моделей}